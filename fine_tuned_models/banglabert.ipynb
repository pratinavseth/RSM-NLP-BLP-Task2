{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# [Sentiment Analysis Shared Task](https://github.com/blp-workshop/blp_task2) at [BLP Workshop](https://blp-workshop.github.io/) @EMNLP 2023\n\nThe main objective of this task is to detect the sentiment associated within a given text. This is a multi-class classification task that involves determining whether the sentiment expressed in the text is Positive, Negative, Neutral.\n\n","metadata":{"id":"Noik9q9c7Bhm","pycharm":{"name":"#%% md\n"}}},{"cell_type":"markdown","source":"### Downloading dataset from github","metadata":{"id":"KSxBhCps7oBf","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_train.tsv\n!wget https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_dev.tsv\n!wget https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_dev_test.tsv","metadata":{"id":"BvwQNYHk6kV5","outputId":"5c70ddfd-5d32-4f1b-f6c5-42c704db45b2","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:35:32.330212Z","iopub.execute_input":"2023-08-14T10:35:32.330491Z","iopub.status.idle":"2023-08-14T10:35:37.069431Z","shell.execute_reply.started":"2023-08-14T10:35:32.330465Z","shell.execute_reply":"2023-08-14T10:35:37.068146Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2023-08-14 10:35:33--  https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_train.tsv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8607028 (8.2M) [text/plain]\nSaving to: ‘blp23_sentiment_train.tsv’\n\nblp23_sentiment_tra 100%[===================>]   8.21M  --.-KB/s    in 0.1s    \n\n2023-08-14 10:35:33 (79.2 MB/s) - ‘blp23_sentiment_train.tsv’ saved [8607028/8607028]\n\n--2023-08-14 10:35:35--  https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_dev.tsv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 942440 (920K) [text/plain]\nSaving to: ‘blp23_sentiment_dev.tsv’\n\nblp23_sentiment_dev 100%[===================>] 920.35K  --.-KB/s    in 0.05s   \n\n2023-08-14 10:35:35 (16.6 MB/s) - ‘blp23_sentiment_dev.tsv’ saved [942440/942440]\n\n--2023-08-14 10:35:36--  https://raw.githubusercontent.com/blp-workshop/blp_task2/main/data/blp23_sentiment_dev_test.tsv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 807595 (789K) [text/plain]\nSaving to: ‘blp23_sentiment_dev_test.tsv’\n\nblp23_sentiment_dev 100%[===================>] 788.67K  --.-KB/s    in 0.05s   \n\n2023-08-14 10:35:36 (14.8 MB/s) - ‘blp23_sentiment_dev_test.tsv’ saved [807595/807595]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### installing required libraries.\n - transformers\n - datasets\n - evaluate\n - accelerate","metadata":{"id":"xYZ96DWt-TZk","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install evaluate\n!pip install --upgrade accelerate","metadata":{"id":"SLJh5GGU-xET","outputId":"df20429a-c21a-441c-eebe-b16b615d71ba","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:35:37.071770Z","iopub.execute_input":"2023-08-14T10:35:37.072191Z","iopub.status.idle":"2023-08-14T10:36:25.619708Z","shell.execute_reply.started":"2023-08-14T10:35:37.072150Z","shell.execute_reply":"2023-08-14T10:36:25.618546Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.30.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.16.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.6.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.3.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.65.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.23.5)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.6.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.4)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nCollecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.20.3)\nCollecting accelerate\n  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nInstalling collected packages: accelerate\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.20.3\n    Uninstalling accelerate-0.20.3:\n      Successfully uninstalled accelerate-0.20.3\nSuccessfully installed accelerate-0.21.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nwandb.login(key='5b96c42987328715d5b09425b4f5836eff23a354')","metadata":{"execution":{"iopub.status.busy":"2023-08-14T10:36:25.622406Z","iopub.execute_input":"2023-08-14T10:36:25.622786Z","iopub.status.idle":"2023-08-14T10:36:28.612881Z","shell.execute_reply.started":"2023-08-14T10:36:25.622750Z","shell.execute_reply":"2023-08-14T10:36:28.611874Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"#### importing required libraries and setting up logger","metadata":{"id":"OXhVWUJ3A_hx","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"import logging\nimport os\nimport random\nimport sys\nfrom dataclasses import dataclass, field\nfrom typing import Optional\nimport pandas as pd\nimport datasets\nimport evaluate\nimport numpy as np\nfrom datasets import load_dataset, Dataset, DatasetDict\nimport torch\n\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    DataCollatorWithPadding,\n    EvalPrediction,\n    HfArgumentParser,\n    PretrainedConfig,\n    Trainer,\n    TrainingArguments,\n    default_data_collator,\n    set_seed,\n)\nfrom transformers.trainer_utils import get_last_checkpoint\nfrom transformers.utils import check_min_version, send_example_telemetry\nfrom transformers.utils.versions import require_version\n\n\nlogger = logging.getLogger(__name__)\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n    datefmt=\"%m/%d/%Y %H:%M:%S\",\n    handlers=[logging.StreamHandler(sys.stdout)],\n)","metadata":{"id":"VIUAU0rRBOmR","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:28.615729Z","iopub.execute_input":"2023-08-14T10:36:28.616317Z","iopub.status.idle":"2023-08-14T10:36:43.500117Z","shell.execute_reply.started":"2023-08-14T10:36:28.616289Z","shell.execute_reply":"2023-08-14T10:36:43.499108Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Defining the training, validation, and test data","metadata":{"id":"HP6CdL7NHpxJ","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train_file = 'blp23_sentiment_train.tsv'\nvalidation_file = 'blp23_sentiment_dev.tsv'\ntest_file = 'blp23_sentiment_dev_test.tsv'","metadata":{"id":"bMzfE34iHyGV","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:43.501705Z","iopub.execute_input":"2023-08-14T10:36:43.502469Z","iopub.status.idle":"2023-08-14T10:36:43.509052Z","shell.execute_reply.started":"2023-08-14T10:36:43.502432Z","shell.execute_reply":"2023-08-14T10:36:43.507951Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Setting up the training parameters","metadata":{"id":"3-_w4YehCgX4","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"training_args = TrainingArguments(\n    learning_rate=2e-5,\n    num_train_epochs=32,\n    per_device_train_batch_size=32,\n    per_device_eval_batch_size=32,\n    output_dir=\"./distilBERT_m/\",\n    overwrite_output_dir=True,\n    remove_unused_columns=False,\n    local_rank= 1,\n    load_best_model_at_end=True,\n    save_total_limit=2,\n    save_strategy=\"no\"\n)\n\nmax_train_samples = None\nmax_eval_samples=None\nmax_predict_samples=None\nmax_seq_length = 512\nbatch_size = 16","metadata":{"id":"7-GUUNj0BPbu","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:43.510642Z","iopub.execute_input":"2023-08-14T10:36:43.511319Z","iopub.status.idle":"2023-08-14T10:36:43.541782Z","shell.execute_reply.started":"2023-08-14T10:36:43.511285Z","shell.execute_reply":"2023-08-14T10:36:43.540902Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"transformers.utils.logging.set_verbosity_info()\n\nlog_level = training_args.get_process_log_level()\nlogger.setLevel(log_level)\ndatasets.utils.logging.set_verbosity(log_level)\ntransformers.utils.logging.set_verbosity(log_level)\ntransformers.utils.logging.enable_default_handler()\ntransformers.utils.logging.enable_explicit_format()\nlogger.warning(\n    f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n    + f\" distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n)\nlogger.info(f\"Training/evaluation parameters {training_args}\")","metadata":{"id":"0Q4deAnUJ0iI","outputId":"653e95ce-b193-401b-a27d-d8309638f699","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:43.542879Z","iopub.execute_input":"2023-08-14T10:36:43.543219Z","iopub.status.idle":"2023-08-14T10:36:43.550785Z","shell.execute_reply.started":"2023-08-14T10:36:43.543187Z","shell.execute_reply":"2023-08-14T10:36:43.549921Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"#### Defining the Model","metadata":{"id":"RgkvwlbFHVo5","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"model_name = 'csebuetnlp/banglabert'","metadata":{"id":"-De1tz5qHYre","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:43.552084Z","iopub.execute_input":"2023-08-14T10:36:43.553137Z","iopub.status.idle":"2023-08-14T10:36:43.563768Z","shell.execute_reply.started":"2023-08-14T10:36:43.553104Z","shell.execute_reply":"2023-08-14T10:36:43.562829Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"#### setting the random seed","metadata":{"id":"yPqrrDbcKN8n","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"set_seed(training_args.seed)","metadata":{"id":"ZvKpoxaQKTB6","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:43.565062Z","iopub.execute_input":"2023-08-14T10:36:43.565449Z","iopub.status.idle":"2023-08-14T10:36:43.575842Z","shell.execute_reply.started":"2023-08-14T10:36:43.565418Z","shell.execute_reply":"2023-08-14T10:36:43.574867Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Loading data files","metadata":{"id":"bgNrs7AhKdvl","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"l2id = {'Positive': 2, 'Neutral': 1, 'Negative': 0}\ntrain_df = pd.read_csv(train_file, sep='\\t')\ntrain_df['label'] = train_df['label'].map(l2id)\ntrain_df = Dataset.from_pandas(train_df)\nvalidation_df = pd.read_csv(validation_file, sep='\\t')\nvalidation_df['label'] = validation_df['label'].map(l2id)\nvalidation_df = Dataset.from_pandas(validation_df)\ntest_df = pd.read_csv(test_file, sep='\\t')\n#test_df['label'] = test_df['label'].map(l2id)\ntest_df = Dataset.from_pandas(test_df)\n\ndata_files = {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\nfor key in data_files.keys():\n    logger.info(f\"loading a local file for {key}\")\nraw_datasets = DatasetDict(\n    {\"train\": train_df, \"validation\": validation_df, \"test\": test_df}\n)","metadata":{"id":"LDwaW8AnKcgD","outputId":"3cc5f195-4053-4628-864b-ac3f200a964d","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:43.580719Z","iopub.execute_input":"2023-08-14T10:36:43.581058Z","iopub.status.idle":"2023-08-14T10:36:43.902857Z","shell.execute_reply.started":"2023-08-14T10:36:43.581035Z","shell.execute_reply":"2023-08-14T10:36:43.901802Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"##### Extracting number of unique labels","metadata":{"id":"BJhNu7tPQ2RU","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"# Labels\nlabel_list = raw_datasets[\"train\"].unique(\"label\")\nlabel_list.sort()  # sort the labels for determine\nnum_labels = len(label_list)","metadata":{"id":"JTl6NNPmOXhO","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:43.904432Z","iopub.execute_input":"2023-08-14T10:36:43.904767Z","iopub.status.idle":"2023-08-14T10:36:43.945874Z","shell.execute_reply.started":"2023-08-14T10:36:43.904734Z","shell.execute_reply":"2023-08-14T10:36:43.944983Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Loading Pretrained Configuration, Tokenizer and Model","metadata":{"id":"J1dpoOAPRJnN","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(\n    model_name,\n    num_labels=num_labels,\n    finetuning_task=None,\n    cache_dir=None,\n    revision=\"main\",\n    use_auth_token=None,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    cache_dir=None,\n    use_fast=True,\n    revision=\"main\",\n    use_auth_token=None,\n)\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    from_tf=bool(\".ckpt\" in model_name),\n    config=config,\n    cache_dir=None,\n    revision=\"main\",\n    use_auth_token=None,\n    ignore_mismatched_sizes=True,\n)","metadata":{"id":"jmAaMuBuRQd2","outputId":"e958d019-4e71-40c6-bec7-bcd619bcfaee","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:43.947708Z","iopub.execute_input":"2023-08-14T10:36:43.948399Z","iopub.status.idle":"2023-08-14T10:36:48.578250Z","shell.execute_reply.started":"2023-08-14T10:36:43.948365Z","shell.execute_reply":"2023-08-14T10:36:48.577331Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f8601c897354f4d996f591c9a03bcd7"}},"metadata":{}},{"name":"stderr","text":"[INFO|configuration_utils.py:669] 2023-08-14 10:36:44,140 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:725] 2023-08-14 10:36:44,156 >> Model config ElectraConfig {\n  \"_name_or_path\": \"csebuetnlp/banglabert\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"id2label\": {\n    \"0\": \"LABEL_0\",\n    \"1\": \"LABEL_1\",\n    \"2\": \"LABEL_2\"\n  },\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"label2id\": {\n    \"LABEL_0\": 0,\n    \"LABEL_1\": 1,\n    \"LABEL_2\": 2\n  },\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.30.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0803ed14751f4320ba87fc17107c0eee"}},"metadata":{}},{"name":"stderr","text":"[INFO|configuration_utils.py:669] 2023-08-14 10:36:44,268 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:725] 2023-08-14 10:36:44,271 >> Model config ElectraConfig {\n  \"_name_or_path\": \"csebuetnlp/banglabert\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.30.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/528k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07a70446681b4791966aeaf62dc61b8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0126de38979433eb0ecfb6327e5f9f0"}},"metadata":{}},{"name":"stderr","text":"[INFO|tokenization_utils_base.py:1823] 2023-08-14 10:36:44,628 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/vocab.txt\n[INFO|tokenization_utils_base.py:1823] 2023-08-14 10:36:44,629 >> loading file tokenizer.json from cache at None\n[INFO|tokenization_utils_base.py:1823] 2023-08-14 10:36:44,630 >> loading file added_tokens.json from cache at None\n[INFO|tokenization_utils_base.py:1823] 2023-08-14 10:36:44,631 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/special_tokens_map.json\n[INFO|tokenization_utils_base.py:1823] 2023-08-14 10:36:44,632 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/tokenizer_config.json\n[INFO|configuration_utils.py:669] 2023-08-14 10:36:44,634 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:725] 2023-08-14 10:36:44,636 >> Model config ElectraConfig {\n  \"_name_or_path\": \"csebuetnlp/banglabert\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.30.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n[INFO|configuration_utils.py:669] 2023-08-14 10:36:44,682 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/config.json\n[INFO|configuration_utils.py:725] 2023-08-14 10:36:44,684 >> Model config ElectraConfig {\n  \"_name_or_path\": \"csebuetnlp/banglabert\",\n  \"architectures\": [\n    \"ElectraForPreTraining\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"embedding_size\": 768,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"max_position_embeddings\": 512,\n  \"model_type\": \"electra\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"summary_activation\": \"gelu\",\n  \"summary_last_dropout\": 0.1,\n  \"summary_type\": \"first\",\n  \"summary_use_proj\": true,\n  \"transformers_version\": \"4.30.2\",\n  \"type_vocab_size\": 2,\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba68fc28c0ab4dfd9dddff654f088a3a"}},"metadata":{}},{"name":"stderr","text":"[INFO|modeling_utils.py:2578] 2023-08-14 10:36:47,059 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--csebuetnlp--banglabert/snapshots/9ce791f330578f50da6bc52b54205166fb5d1c8c/pytorch_model.bin\n[WARNING|modeling_utils.py:3285] 2023-08-14 10:36:48,564 >> Some weights of the model checkpoint at csebuetnlp/banglabert were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias']\n- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n[WARNING|modeling_utils.py:3297] 2023-08-14 10:36:48,565 >> Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Preprocessing the raw_datasets","metadata":{"id":"m7PIQVypeTf4","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\nsentence1_key= non_label_column_names[1]\n\n# Padding strategy\npadding = \"max_length\"\n\n# Some models have set the order of the labels to use, so let's make sure we do use it.\nlabel_to_id = None\nif (model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id):\n    # Some have all caps in their config, some don't.\n    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n    if sorted(label_name_to_id.keys()) == sorted(label_list):\n        label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n    else:\n        logger.warning(\n            \"Your model seems to have been trained with labels, but they don't match the dataset: \",\n            f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n            \"\\nIgnoring the model labels as a result.\",)\n\nif label_to_id is not None:\n    model.config.label2id = label_to_id\n    model.config.id2label = {id: label for label, id in config.label2id.items()}\n\nif 128 > tokenizer.model_max_length:\n    logger.warning(\n        f\"The max_seq_length passed ({128}) is larger than the maximum length for the\"\n        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\")\nmax_seq_length = min(128, tokenizer.model_max_length)\n\ndef preprocess_function(examples):\n    # Tokenize the texts\n    args = (\n        (examples[sentence1_key],))\n    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n\n    # Map labels to IDs (not necessary for GLUE tasks)\n    if label_to_id is not None and \"label\" in examples:\n        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n    return result\nraw_datasets = raw_datasets.map(\n    preprocess_function,\n    batched=True,\n    load_from_cache_file=True,\n    desc=\"Running tokenizer on dataset\",\n)\n","metadata":{"id":"pqO3YWAZelhd","outputId":"0d57deee-f1bc-4983-e945-e3bfdbd98765","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:48.579898Z","iopub.execute_input":"2023-08-14T10:36:48.580322Z","iopub.status.idle":"2023-08-14T10:36:56.819923Z","shell.execute_reply.started":"2023-08-14T10:36:48.580286Z","shell.execute_reply":"2023-08-14T10:36:56.818907Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/36 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531557b41cab4340b443094dbd49377e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e160386a95b64e1394925b7248c82696"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7f0b11c532b4096943020c015f0801d"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### Finalize the training data for training the model","metadata":{"id":"ASxWKiqifb_g","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"if \"train\" not in raw_datasets:\n    raise ValueError(\"requires a train dataset\")\ntrain_dataset = raw_datasets[\"train\"]\nif max_train_samples is not None:\n    max_train_samples_n = min(len(train_dataset), max_train_samples)\n    train_dataset = train_dataset.select(range(max_train_samples_n))","metadata":{"id":"QHoDqrBGgD6F","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:56.821727Z","iopub.execute_input":"2023-08-14T10:36:56.822472Z","iopub.status.idle":"2023-08-14T10:36:56.828147Z","shell.execute_reply.started":"2023-08-14T10:36:56.822430Z","shell.execute_reply":"2023-08-14T10:36:56.827083Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"id":"FqME25nm-hwo","outputId":"7ad2220b-e9e2-4553-ef82-6b5e769155a0","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:56.829694Z","iopub.execute_input":"2023-08-14T10:36:56.830366Z","iopub.status.idle":"2023-08-14T10:36:58.442976Z","shell.execute_reply.started":"2023-08-14T10:36:56.830330Z","shell.execute_reply":"2023-08-14T10:36:58.441599Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 35266\n})"},"metadata":{}}]},{"cell_type":"markdown","source":"#### Finalize the development/evaluation data for evaluating the model","metadata":{"id":"k72vUTSigOzZ","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"if \"validation\" not in raw_datasets:\n    raise ValueError(\"requires a validation dataset\")\neval_dataset = raw_datasets[\"validation\"]\nif max_eval_samples is not None:\n    max_eval_samples_n = min(len(eval_dataset), max_eval_samples)\n    eval_dataset = eval_dataset.select(range(max_eval_samples_n))","metadata":{"id":"MqrW8ospgUYZ","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:58.444746Z","iopub.execute_input":"2023-08-14T10:36:58.445169Z","iopub.status.idle":"2023-08-14T10:36:58.454680Z","shell.execute_reply.started":"2023-08-14T10:36:58.445136Z","shell.execute_reply":"2023-08-14T10:36:58.453721Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"#### Finalize the test data for predicting the unseen test data using the model","metadata":{"id":"B7sVqp3hgU4i","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"if \"test\" not in raw_datasets and \"test_matched\" not in raw_datasets:\n    raise ValueError(\"requires a test dataset\")\npredict_dataset = raw_datasets[\"test\"]\nif max_predict_samples is not None:\n    max_predict_samples_n = min(len(predict_dataset), max_predict_samples)\n    predict_dataset = predict_dataset.select(range(max_predict_samples_n))","metadata":{"id":"u0dBjIQggcYs","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:58.455923Z","iopub.execute_input":"2023-08-14T10:36:58.456362Z","iopub.status.idle":"2023-08-14T10:36:58.473826Z","shell.execute_reply.started":"2023-08-14T10:36:58.456329Z","shell.execute_reply":"2023-08-14T10:36:58.472907Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"#### Log a few random samples from the training set","metadata":{"id":"Cqbo1xzRge36","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"for index in random.sample(range(len(train_dataset)), 3):\n    logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")","metadata":{"id":"wIO2bxSVgkLb","outputId":"f1883155-7dd1-4a0e-802d-47232826c05c","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:58.475427Z","iopub.execute_input":"2023-08-14T10:36:58.475884Z","iopub.status.idle":"2023-08-14T10:36:58.486936Z","shell.execute_reply.started":"2023-08-14T10:36:58.475852Z","shell.execute_reply":"2023-08-14T10:36:58.485653Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"#### Get the metric function `accuracy`","metadata":{"id":"nAcn0Pc8gogF","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"metric = evaluate.load(\"accuracy\")","metadata":{"id":"aMWMQdaUgvAq","outputId":"f26a55bc-42ac-4759-8628-edcadf66a827","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:58.488416Z","iopub.execute_input":"2023-08-14T10:36:58.488826Z","iopub.status.idle":"2023-08-14T10:36:58.839907Z","shell.execute_reply.started":"2023-08-14T10:36:58.488794Z","shell.execute_reply":"2023-08-14T10:36:58.839018Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f09b8a3b922f48bf9d03207c09635efa"}},"metadata":{}}]},{"cell_type":"markdown","source":"#### Predictions and label_ids field and has to return a dictionary string to float.","metadata":{"id":"foWUyuBHgxbA","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n    preds = np.argmax(preds, axis=1)\n    return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n","metadata":{"id":"-3VqxkqcgxCC","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:58.841432Z","iopub.execute_input":"2023-08-14T10:36:58.841824Z","iopub.status.idle":"2023-08-14T10:36:58.848149Z","shell.execute_reply.started":"2023-08-14T10:36:58.841789Z","shell.execute_reply":"2023-08-14T10:36:58.846828Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"#### Data Collator","metadata":{"id":"dNWK1Hfbg8-o","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"data_collator = default_data_collator","metadata":{"id":"_w6lNh-OhJLC","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:58.849693Z","iopub.execute_input":"2023-08-14T10:36:58.850045Z","iopub.status.idle":"2023-08-14T10:36:58.859872Z","shell.execute_reply.started":"2023-08-14T10:36:58.850012Z","shell.execute_reply":"2023-08-14T10:36:58.858970Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"#### Initialize our Trainer","metadata":{"id":"2nYlugPRhNbg","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    compute_metrics=compute_metrics,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)","metadata":{"id":"yeJco0JOhPHx","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:36:58.861319Z","iopub.execute_input":"2023-08-14T10:36:58.861726Z","iopub.status.idle":"2023-08-14T10:37:03.932624Z","shell.execute_reply.started":"2023-08-14T10:36:58.861694Z","shell.execute_reply":"2023-08-14T10:37:03.931630Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Training our model","metadata":{"id":"cUxWn9HrhqRM","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"train_result = trainer.train()\nmetrics = train_result.metrics\nmax_train_samples = (\n    max_train_samples if max_train_samples is not None else len(train_dataset)\n)\nmetrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n\n","metadata":{"id":"B681qnPFhtY0","outputId":"ad31b0c1-f257-4e60-f981-4a852284ee1b","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:37:03.934259Z","iopub.execute_input":"2023-08-14T10:37:03.934622Z","iopub.status.idle":"2023-08-14T10:39:58.883718Z","shell.execute_reply.started":"2023-08-14T10:37:03.934588Z","shell.execute_reply":"2023-08-14T10:39:58.880448Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n[INFO|trainer.py:1786] 2023-08-14 10:37:04,321 >> ***** Running training *****\n[INFO|trainer.py:1787] 2023-08-14 10:37:04,322 >>   Num examples = 35,266\n[INFO|trainer.py:1788] 2023-08-14 10:37:04,323 >>   Num Epochs = 32\n[INFO|trainer.py:1789] 2023-08-14 10:37:04,324 >>   Instantaneous batch size per device = 32\n[INFO|trainer.py:1790] 2023-08-14 10:37:04,325 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n[INFO|trainer.py:1791] 2023-08-14 10:37:04,326 >>   Gradient Accumulation steps = 1\n[INFO|trainer.py:1792] 2023-08-14 10:37:04,327 >>   Total optimization steps = 35,296\n[INFO|trainer.py:1793] 2023-08-14 10:37:04,330 >>   Number of trainable parameters = 110,619,651\n[INFO|integrations.py:727] 2023-08-14 10:37:04,337 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkomlixmathur\u001b[0m (\u001b[33memnlp_blpt2_23\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.15.8 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20230814_103704-gzx69plq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/emnlp_blpt2_23/huggingface/runs/gzx69plq' target=\"_blank\">stellar-dream-21</a></strong> to <a href='https://wandb.ai/emnlp_blpt2_23/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/emnlp_blpt2_23/huggingface' target=\"_blank\">https://wandb.ai/emnlp_blpt2_23/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/emnlp_blpt2_23/huggingface/runs/gzx69plq' target=\"_blank\">https://wandb.ai/emnlp_blpt2_23/huggingface/runs/gzx69plq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='363' max='35296' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  363/35296 02:18 < 3:43:24, 2.61 it/s, Epoch 0.33/32]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n\u001b[1;32m      3\u001b[0m max_train_samples \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      4\u001b[0m     max_train_samples \u001b[38;5;28;01mif\u001b[39;00m max_train_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(train_dataset)\n\u001b[1;32m      5\u001b[0m )\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1938\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1943\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1944\u001b[0m ):\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2770\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2768\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2770\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:1853\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1853\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"#### Saving the tokenizer too for easy upload","metadata":{"id":"SaaRglkwllSp","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"trainer.save_model()\ntrainer.log_metrics(\"train\", metrics)\ntrainer.save_metrics(\"train\", metrics)\ntrainer.save_state()","metadata":{"id":"9UwoMEbAloMx","outputId":"4e0c3878-1ab8-46fb-b8d0-2459e15dbca0","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:39:58.887055Z","iopub.status.idle":"2023-08-14T10:39:58.889356Z","shell.execute_reply.started":"2023-08-14T10:39:58.889085Z","shell.execute_reply":"2023-08-14T10:39:58.889112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Evaluating our model on validation/development data","metadata":{"id":"K9zCKBGEhwb7","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"logger.info(\"*** Evaluate ***\")\n\nmetrics = trainer.evaluate(eval_dataset=eval_dataset)\n\nmax_eval_samples = (\n    max_eval_samples if max_eval_samples is not None else len(eval_dataset)\n)\nmetrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n\ntrainer.log_metrics(\"eval\", metrics)\ntrainer.save_metrics(\"eval\", metrics)","metadata":{"id":"YClw3dXTh17u","outputId":"09e8bbbb-9616-4661-f0f0-7e415fe0f6c5","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:39:58.890707Z","iopub.status.idle":"2023-08-14T10:39:58.891349Z","shell.execute_reply.started":"2023-08-14T10:39:58.891105Z","shell.execute_reply":"2023-08-14T10:39:58.891128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Predecting the test data","metadata":{"id":"Y3LSdUdPh7uG","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"id2l = {0:'Negative', 1:'Neutral', 2:'Positive'}\nlogger.info(\"*** Predict ***\")\n#predict_dataset = predict_dataset.remove_columns(\"label\")\nids = predict_dataset['id']\npredict_dataset = predict_dataset.remove_columns(\"id\")\npredictions = trainer.predict(predict_dataset, metric_key_prefix=\"predict\").predictions\npredictions = np.argmax(predictions, axis=1)\noutput_predict_file = os.path.join(training_args.output_dir, f\"predict_results.tsv\")\nif trainer.is_world_process_zero():\n    with open(output_predict_file, \"w\") as writer:\n        logger.info(f\"***** Predict results *****\")\n        writer.write(\"id\\tlabel\\n\")\n        for index, item in enumerate(predictions):\n            item = label_list[item]\n            item = id2l[item]\n            writer.write(f\"{ids[index]}\\t{item}\\n\")","metadata":{"id":"gnXhVq6Yh_oS","outputId":"e0a3285b-087e-4caa-940f-dac4d668f19c","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:39:58.893157Z","iopub.status.idle":"2023-08-14T10:39:58.893629Z","shell.execute_reply.started":"2023-08-14T10:39:58.893382Z","shell.execute_reply":"2023-08-14T10:39:58.893404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids[0]","metadata":{"id":"8Gqqk_24__47","outputId":"271068c7-8172-4222-f112-4f7d24da3f31","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:39:58.895256Z","iopub.status.idle":"2023-08-14T10:39:58.895743Z","shell.execute_reply.started":"2023-08-14T10:39:58.895483Z","shell.execute_reply":"2023-08-14T10:39:58.895507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Saving the model into card","metadata":{"id":"fQgoTTIoiI0X","pycharm":{"name":"#%% md\n"}}},{"cell_type":"code","source":"kwargs = {\"finetuned_from\": model_name, \"tasks\": \"text-classification\"}\ntrainer.create_model_card(**kwargs)","metadata":{"id":"B1ooJgrViLVj","pycharm":{"name":"#%%\n"},"execution":{"iopub.status.busy":"2023-08-14T10:39:58.897373Z","iopub.status.idle":"2023-08-14T10:39:58.897837Z","shell.execute_reply.started":"2023-08-14T10:39:58.897603Z","shell.execute_reply":"2023-08-14T10:39:58.897626Z"},"trusted":true},"execution_count":null,"outputs":[]}]}